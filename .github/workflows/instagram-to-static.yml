name: Instagram → Static media.json
on:
  schedule: [{ cron: "0 3,15 * * *" }]   # run twice/day to dodge rate limits
  workflow_dispatch: {}                  # allow manual run

permissions:
  contents: write

jobs:
  build-media-json:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout main
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Instaloader
        run: pip install instaloader

      - name: Gentle backoff (random 20–90s)
        run: python - << 'PY'
import random, time
time.sleep(random.randint(20,90))
print("Slept to avoid burst scraping.")
PY

      # Anonymous scrape. It may be rate-limited; that's OK — we'll keep old media.json.
      - name: Fetch latest posts (anonymous)
        shell: bash
        run: |
          rm -rf scraped && mkdir -p scraped
          # smaller count -> fewer requests -> fewer 401/403s
          instaloader --no-video-thumbnails --no-compress-json \
            --dirname-pattern scraped --fast-update \
            --count 10 "ravandeh.std" || true
          echo "Scraped tree:"
          ls -la scraped || true

      - name: Build media.json (only if we actually got posts)
        id: build
        shell: bash
        run: |
          python - << 'PY'
import json, os, glob, datetime, re, sys

def post_dirs(root="scraped"):
    return sorted([d for d in glob.glob(os.path.join(root, "*")) if os.path.isdir(d)])

def read_meta(postdir):
    for p in sorted(glob.glob(os.path.join(postdir, "*.json"))):
        try:
            import json; return json.load(open(p, "r", encoding="utf-8"))
        except Exception: pass
    return {}

items = []
for d in post_dirs():
    imgs = sorted([p for p in os.listdir(d) if p.lower().endswith((".jpg",".jpeg",".png",".webp"))])
    if not imgs: 
        continue
    img_path = f"{d}/{imgs[0]}".replace("\\","/")
    meta = read_meta(d); node = meta.get("node", {})
    shortcode = meta.get("shortcode") or node.get("shortcode") or ""
    edges = node.get("edge_media_to_caption", {}).get("edges", [])
    caption = edges[0]["node"].get("text","") if edges and edges[0].get("node") else (meta.get("caption","") or "")
    ts = meta.get("taken_at_timestamp") or meta.get("date_local") or meta.get("date_utc")
    if isinstance(ts, (int, float)):
        timestamp = datetime.datetime.utcfromtimestamp(ts).isoformat() + "Z"
    else:
        m = re.search(r"(20\\d{2}-\\d{2}-\\d{2})", d)
        if m:
            try:
                timestamp = datetime.datetime.fromisoformat(m.group(1)).isoformat() + "Z"
            except Exception:
                timestamp = datetime.datetime.utcnow().isoformat() + "Z"
        else:
            timestamp = datetime.datetime.utcnow().isoformat() + "Z"
    items.append({
        "id": meta.get("id") or shortcode or img_path,
        "caption": caption,
        "media_url": img_path,  # RELATIVE path (works on GH Pages)
        "permalink": f"https://www.instagram.com/p/{shortcode}/" if shortcode else "https://www.instagram.com/ravandeh.std/",
        "media_type": "IMAGE",
        "timestamp": timestamp
    })

items.sort(key=lambda x: x["timestamp"], reverse=True)

if items:
    json.dump(items, open("media.json","w",encoding="utf-8"), ensure_ascii=False, indent=2)
    print(f"Wrote media.json with {len(items)} items.")
    print("HAS_CHANGES=true")
else:
    print("No posts scraped (rate limit?). Keeping existing media.json.")
    print("HAS_CHANGES=false")
PY
          # Export a boolean output for the next step
          HAS_CHANGES=$(python - << 'PY'
import sys, re
log = open(0).read() if False else ""
# The above trick won't work; instead just echo from python and grep below. (handled in bash)
PY
          )
        continue-on-error: false

      - name: Did we produce a new media.json?
        id: flag
        shell: bash
        run: |
          if [ -f media.json ]; then
            echo "has_changes=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_changes=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Commit & push (only if new data)
        if: steps.flag.outputs.has_changes == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "Update media.json + scraped images" || echo "No changes to commit"
          git push origin HEAD:main

      - name: Log note when keeping previous media.json
        if: steps.flag.outputs.has_changes != 'true'
        run: echo "No fresh posts scraped; kept previous media.json (site remains up)."
