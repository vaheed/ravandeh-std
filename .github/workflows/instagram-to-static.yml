name: Instagram -> Static media.json

on:
  schedule:
    - cron: "0 3,15 * * *"   # run twice per day
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout main
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Instaloader
        run: pip install instaloader

      - name: Gentle backoff
        run: bash -lc 'sleep $(( (RANDOM % 71) + 20 ))'

      - name: Fetch latest posts (anonymous)
        run: |
          rm -rf scraped && mkdir -p scraped
          # Smaller count reduces rate-limit risk. If IG blocks, we still continue.
          instaloader --no-video-thumbnails --no-compress-json \
            --dirname-pattern scraped --fast-update \
            --count 10 "ravandeh.std" || true
          echo "Scraped tree:"
          ls -la scraped || true

      - name: Build media.new (relative paths)
        run: |
          python <<'PY'
import json, os, glob, datetime, re

def post_dirs(root="scraped"):
    return sorted([d for d in glob.glob(os.path.join(root, "*")) if os.path.isdir(d)])

def read_meta(d):
    for p in sorted(glob.glob(os.path.join(d, "*.json"))):
        try:
            with open(p, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {}

items = []
for d in post_dirs():
    imgs = sorted([p for p in os.listdir(d) if p.lower().endswith((".jpg",".jpeg",".png",".webp"))])
    if not imgs:
        continue
    img_path = f"{d}/{imgs[0]}".replace("\\", "/")
    meta = read_meta(d)
    node = meta.get("node", {})
    shortcode = meta.get("shortcode") or node.get("shortcode") or ""
    edges = node.get("edge_media_to_caption", {}).get("edges", [])
    caption = edges[0]["node"].get("text","") if edges and edges[0].get("node") else (meta.get("caption","") or "")
    ts = meta.get("taken_at_timestamp") or meta.get("date_local") or meta.get("date_utc")
    if isinstance(ts, (int, float)):
        timestamp = datetime.datetime.utcfromtimestamp(ts).isoformat() + "Z"
    else:
        m = re.search(r"(20\\d{2}-\\d{2}-\\d{2})", d)
        if m:
            try:
                timestamp = datetime.datetime.fromisoformat(m.group(1)).isoformat() + "Z"
            except Exception:
                timestamp = datetime.datetime.utcnow().isoformat() + "Z"
        else:
            timestamp = datetime.datetime.utcnow().isoformat() + "Z"

    items.append({
        "id": meta.get("id") or shortcode or img_path,
        "caption": caption,
        "media_url": img_path,  # RELATIVE path (works on GitHub Pages)
        "permalink": f"https://www.instagram.com/p/{shortcode}/" if shortcode else "https://www.instagram.com/ravandeh.std/",
        "media_type": "IMAGE",
        "timestamp": timestamp
    })

items.sort(key=lambda x: x["timestamp"], reverse=True)

if items:
    with open("media.new","w",encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)
else:
    # No posts scraped (rate limit or none found) -> don't write anything.
    pass
PY

      - name: Commit media.json if we have new data
        run: |
          if [ -f media.new ]; then
            mv media.new media.json
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add -A
            git commit -m "Update media.json + scraped images" || echo "No changes to commit"
            git push origin HEAD:main
          else
            echo "No fresh posts scraped; keeping previous media.json."
          fi
